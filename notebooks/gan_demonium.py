# -*- coding: utf-8 -*-
"""GAN_demonium.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jkCfhUwrJtoh3M4X2jC4mevv3HoDZju2

Assuming you have the Repo in your Goolge Drive - we will mount Collab to the project's directory
"""

# you will be prompted with a window asking to grant permissions
from google.colab import drive
drive.mount("/content/drive")

"""You can git clone the repo and save it to disk if you don't want to save the repo in your Goolgle Drive"""

# !git clone https://github.com/SuP3RM/DualStyleGAN.git $CODE_DIR

# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces
import os
datadir = "/content/drive/MyDrive/Collab_Notebooks/CS_543/DualStyleGAN"
if not os.path.exists(datadir):
  !ln -s $datadir # TODO: Fill your A3 path
os.chdir(datadir)
!pwd

print(datadir)

os.environ['CUDA_VISIBLE_DEVICES'] = "0"
# os.chdir('../')
CODE_DIR = datadir
device = 'cuda'

"""Download [ninja](https://pypi.org/project/ninja/) a small build system with a focus on speed"""

!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip
!sudo unzip ninja-linux.zip -d /usr/local/bin/
!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force

"""**faiss-cpu** is a [library](https://pypi.org/project/faiss-cpu/) for efficient similarity search and clustering of dense vectors"""

!pip install faiss-cpu
!pip install wget

# os.chdir(f'./{CODE_DIR}')
MODEL_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'checkpoint/')
DATA_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'data/')
print(MODEL_DIR)
print(DATA_DIR)
!pwd

"""### Import libraries ðŸ“š """

!pip install wandb argparse

!pip install lmdb # install needed for fine tuning

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

import sys
sys.path.append(".")
sys.path.append("..")

import numpy as np
import torch
from util import save_image, load_image, visualize
import argparse
from argparse import Namespace
from torchvision import transforms
from torch.nn import functional as F
import torch.utils.checkpoint
import torchvision
import matplotlib.pyplot as plt
from model.dualstylegan import DualStyleGAN
from model.sampler.icp import ICPTrainer
from model.encoder.psp import pSp

torch.version.cuda

torch.__version__

torch.cuda.empty_cache() # clear cache
torch.nn.parallel.DistributedDataParallel

# max_split_size_mb = '2GiB'

# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize

import psutil
import humanize
import os
import GPUtil as GPU

GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
    process = psutil.Process(os.getpid())
    print("Gen RAM Free: " + humanize.naturalsize(psutil.virtual_memory().available), " |     Proc size: " + humanize.naturalsize(process.memory_info().rss))
    print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

"""## Step 1: Select Available Style Types"""

# style_types = ['cartoon', 'caricature', 'anime', 'arcane', 'comic', 'pixar', 'slamdunk']
# style_type = style_types[0]

our_style_types = ['cartoon', 'avatar', 'face_cartoon', 'cartoon_images_augment', 'fantasy', 'impasto']
style_type = our_style_types[0]

"""## Step 2: Set Pretrained Models Mapping

"""

MODEL_PATHS = {
    "encoder": {"id": "1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej", "name": "encoder.pt"},
    "cartoon-G": {"id": "1exS9cSFkg8J4keKPmq2zYQYfJYC5FkwL", "name": "generator.pt"},
    "cartoon-N": {"id": "1JSCdO0hx8Z5mi5Q5hI9HMFhLQKykFX5N", "name": "sampler.pt"},
    "cartoon-S": {"id": "1ce9v69JyW_Dtf7NhbOkfpH77bS_RK0vB", "name": "refined_exstyle_code.npy"},
    "caricature-G": {"id": "1BXfTiMlvow7LR7w8w0cNfqIl-q2z0Hgc", "name": "generator.pt"},
    "caricature-N": {"id": "1eJSoaGD7X0VbHS47YLehZayhWDSZ4L2Q", "name": "sampler.pt"},
    "caricature-S": {"id": "1-p1FMRzP_msqkjndRK_0JasTdwQKDsov", "name": "refined_exstyle_code.npy"},
    "anime-G": {"id": "1BToWH-9kEZIx2r5yFkbjoMw0642usI6y", "name": "generator.pt"},
    "anime-N": {"id": "19rLqx_s_SUdiROGnF_C6_uOiINiNZ7g2", "name": "sampler.pt"},
    "anime-S": {"id": "17-f7KtrgaQcnZysAftPogeBwz5nOWYuM", "name": "refined_exstyle_code.npy"},
    "arcane-G": {"id": "15l2O7NOUAKXikZ96XpD-4khtbRtEAg-Q", "name": "generator.pt"},
    "arcane-N": {"id": "1fa7p9ZtzV8wcasPqCYWMVFpb4BatwQHg", "name": "sampler.pt"},
    "arcane-S": {"id": "1z3Nfbir5rN4CrzatfcgQ8u-x4V44QCn1", "name": "exstyle_code.npy"},
    "comic-G": {"id": "1_t8lf9lTJLnLXrzhm7kPTSuNDdiZnyqE", "name": "generator.pt"},
    "comic-N": {"id": "1RXrJPodIn7lCzdb5BFc03kKqHEazaJ-S", "name": "sampler.pt"},
    "comic-S": {"id": "1ZfQ5quFqijvK3hO6f-YDYJMqd-UuQtU-", "name": "exstyle_code.npy"},
    "pixar-G": {"id": "1TgH7WojxiJXQfnCroSRYc7BgxvYH9i81", "name": "generator.pt"},
    "pixar-N": {"id": "18e5AoQ8js4iuck7VgI3hM_caCX5lXlH_", "name": "sampler.pt"},
    "pixar-S": {"id": "1I9mRTX2QnadSDDJIYM_ntyLrXjZoN7L-", "name": "exstyle_code.npy"},    
    "slamdunk-G": {"id": "1MGGxSCtyf9399squ3l8bl0hXkf5YWYNz", "name": "generator.pt"},
    "slamdunk-N": {"id": "1-_L7YVb48sLr_kPpOcn4dUq7Cv08WQuG", "name": "sampler.pt"},
    "slamdunk-S": {"id": "1Dgh11ZeXS2XIV2eJZAExWMjogxi_m_C8", "name": "exstyle_code.npy"},
    # "avatar-S": {"id": "", "name": "refined_exstyle_code.npy"}
}

# MODEL_PATHS = {
#     "encoder": {"id": "1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej", "name": "encoder.pt"},
# }

"""## Step 3: Load Pretrained Model
We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary.
"""

transform = transforms.Compose(
    [
        transforms.Resize(256),
        transforms.CenterCrop(256),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
    ]
)

# load DualStyleGAN
generator = DualStyleGAN(1024, 512, 8, 2, res_index=6) # add another ModResBlock ? 
generator.eval()
ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'generator.pt'))
generator.load_state_dict(ckpt["g_ema"])
generator = generator.to(device)

# load encoder
model_path = os.path.join(MODEL_DIR, 'encoder.pt')
ckpt = torch.load(model_path, map_location='cpu')
opts = ckpt['opts']
opts['checkpoint_path'] = model_path
opts = Namespace(**opts)
opts.device = device
encoder = pSp(opts)
encoder.eval()
encoder = encoder.to(device)

# uncomment to load extrinsic style code
# ePath = os.path.join(MODEL_DIR, style_type, 'exstyle_code.npy')
# print(ePath)
# exstyles = np.load(ePath, allow_pickle=True)

# cartoon
exstyles = np.load(os.path.join(MODEL_DIR, style_type, MODEL_PATHS[style_type+'-S']["name"]), allow_pickle='TRUE').item()

# load sampler network
icptc = ICPTrainer(np.empty([0,512*11]), 128)
icpts = ICPTrainer(np.empty([0,512*7]), 128)
ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'sampler.pt'), map_location=lambda storage, loc: storage)
icptc.icp.netT.load_state_dict(ckpt['color'])
icpts.icp.netT.load_state_dict(ckpt['structure'])
icptc.icp.netT = icptc.icp.netT.to(device)
icpts.icp.netT = icpts.icp.netT.to(device)

print('Model successfully loaded!')

"""## Step 5: Visualize Input

Try your image from the `notebooks/images` directory
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# image_path = './checkpoint/content/unsplash-rDEOVtE7vOs.jpg'
# original_image = load_image(image_path)

image_path = './notebooks/images/input_img.jpg'
original_image = load_image(image_path)

plt.figure(figsize=(10,10),dpi=30)
visualize(original_image[0])
plt.show()

"""### Align Image

Note: Our style transfer assumes the input has been pre-aligned.
If the original image is not pre-aligned, please run the following alignment scripts.
"""

if_align_face = True

def run_alignment(image_path):
    import dlib
    from model.encoder.align_all_parallel import align_face
    modelname = os.path.join(MODEL_DIR, 'shape_predictor_68_face_landmarks.dat')
    if not os.path.exists(modelname):
        import wget, bz2
        wget.download('http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2', modelname+'.bz2')
        zipfile = bz2.BZ2File(modelname+'.bz2')
        data = zipfile.read()
        open(modelname, 'wb').write(data) 
    predictor = dlib.shape_predictor(modelname)
    aligned_image = align_face(filepath=image_path, predictor=predictor)
    return aligned_image

if if_align_face:
    I = transform(run_alignment(image_path)).unsqueeze(dim=0).to(device)
else:
    I = F.adaptive_avg_pool2d(load_image(image_path).to(device), 256)

plt.figure(figsize=(10,10),dpi=30)
visualize(I[0].cpu())
plt.show()

"""## Step 6: Perform Inference -- Style Transfer

### Select style image

Select the style id.
We assume that you have downloaded the dataset and placed them in `./data/STYLE_TYPE/images/train/`.
If not, the style images will not be shown below.

Original Code
"""

# stylename = 'cs11577691271385561071.jpg' # avatar dataset some examples: ['cs11577684247192164878', 'cs11577676438302104026']

# uncomment to try to load the style image for Cartoon dataset
style_id = 2
stylename = list(exstyles.keys())[style_id]

stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename)
print('loading %s'%stylepath)
if os.path.exists(stylepath):
    S = load_image(stylepath)
    plt.figure(figsize=(10,10),dpi=30)
    visualize(S[0])
    plt.show()
else:
    print('%s is not found'%stylename)

"""Manually test with uploaded images"""

# # testing with manually upload images
# stylename = 'goku_black' + '.jpg'
# stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename)
# print('loading %s'%stylepath)
# if os.path.exists(stylepath):
#     S = load_image(stylepath)
#     plt.figure(figsize=(10,10),dpi=30)
#     visualize(S[0])
#     plt.show()
# else:
#     print('%s is not found'%stylename)

"""### Style transfer with and without color preservation"""

with torch.no_grad():
    img_rec, instyle = encoder(I, randomize_noise=False, return_latents=True, 
                            z_plus_latent=True, return_z_plus_latent=True, resize=False)    
    img_rec = torch.clamp(img_rec.detach(), -1, 1)

    latent = torch.tensor(exstyles[stylename]).repeat(2,1,1).to(device)
    # latent[0] for both color and structrue transfer and latent[1] for only structrue transfer
    latent[1,7:18] = instyle[0,7:18]
    exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)
    
    img_gen, _ = generator([instyle.repeat(2,1,1)], exstyle, z_plus_latent=True, 
                           truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)
    img_gen = torch.clamp(img_gen.detach(), -1, 1)
    # deactivate color-related layers by setting w_c = 0
    img_gen2, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, 
                            truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[0]*11)
    img_gen2 = torch.clamp(img_gen2.detach(), -1, 1)

"""### Visualize Results
From left to right:
1. **pSp recontructed content image**
2. **style transfer result**: both color and strcture styles are transferred
3. **structure transfer result**: preserve the color of the content image by replacing the extrinsic color codes with intrinsic color codes
4. **structure transfer result**: preserve the color of the content image by deactivating color-related layers
"""

vis = torchvision.utils.make_grid(F.adaptive_avg_pool2d(torch.cat([img_rec, img_gen, img_gen2], dim=0), 256), 4, 1)
plt.figure(figsize=(10,10),dpi=120)
visualize(vis.cpu())
plt.show()

"""### Navigation with different interp_weights to achieve flexible style manipulation

Users are suggested to try different interp_weights to find the ideal results
"""

results = []
for i in range(6): # change weights of structure codes 
    for j in range(6): # change weights of color codes
        w = [i/5.0]*7+[j/5.0]*11

        img_gen, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, 
                                truncation=0.7, truncation_latent=0, use_res=True, interp_weights=w)
        img_gen = torch.clamp(F.adaptive_avg_pool2d(img_gen.detach(), 128), -1, 1)
        results += [img_gen]
        
vis = torchvision.utils.make_grid(torch.cat(results, dim=0), 6, 1)
plt.figure(figsize=(10,10),dpi=120)
visualize(vis.cpu())
plt.show()

"""Style fusion"""

style_id2 = 53
# try to load the style image
stylename2 = list(exstyles.keys())[style_id2]
stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename2)
print('loading %s'%stylepath)
if os.path.exists(stylepath):
    S = load_image(stylepath)
    plt.figure(figsize=(10,10),dpi=30)
    visualize(S[0])
    plt.show()
else:
    print('%s is not found'%stylename2)

with torch.no_grad():
    latent = torch.tensor(exstyles[stylename]).repeat(6,1,1).to(device)
    latent2 = torch.tensor(exstyles[stylename2]).repeat(6,1,1).to(device)
    fuse_weight = torch.arange(6).reshape(6,1,1).to(device) / 5.0
    fuse_latent = latent * fuse_weight + latent2 * (1-fuse_weight)
    exstyle = generator.generator.style(fuse_latent.reshape(fuse_latent.shape[0]*fuse_latent.shape[1], fuse_latent.shape[2])).reshape(fuse_latent.shape)
    
    img_gen, _ = generator([instyle.repeat(6,1,1)], exstyle, z_plus_latent=True, 
                           truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)
    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)
    
vis = torchvision.utils.make_grid(img_gen, 6, 1)
plt.figure(figsize=(10,10),dpi=120)
visualize(vis.cpu())
plt.show()

"""## Step 7: Perform Inference -- Artistic Portrait Generation

### Randomly sample both intrinsic and extrinsic style codes
"""

seed = 123
torch.manual_seed(seed)
batch = 6 # sample 6 style codes

with torch.no_grad():
    instyle = torch.randn(6, 512).to(device)
    # sample structure codes
    res_in = icpts.icp.netT(torch.randn(batch, 128).to(device)).reshape(-1,7,512)
    # sample color codes
    ada_in = icptc.icp.netT(torch.randn(batch, 128).to(device)).reshape(-1,11,512)

    # concatenate two codes to form the complete extrinsic style code
    latent = torch.cat((res_in, ada_in), dim=1)
    # map into W+ space
    exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)

with torch.no_grad():
    img_gen, _ = generator([instyle], exstyle, input_is_latent=False, truncation=0.7, truncation_latent=0, 
                           use_res=True, interp_weights=[0.6]*7+[1]*11)
    
    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)
    
vis = torchvision.utils.make_grid(img_gen, batch, 1)
plt.figure(figsize=(10,10),dpi=120)
visualize(vis.cpu())
plt.show()

"""### Fix extrinsic style codes"""

with torch.no_grad():
    img_gen, _ = generator([instyle], exstyle[4:5].repeat(batch, 1, 1), input_is_latent=False, truncation=0.7, truncation_latent=0, 
                           use_res=True, interp_weights=[0.6]*7+[1]*11)
    
    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)
    
vis = torchvision.utils.make_grid(img_gen, batch, 1)
plt.figure(figsize=(10,10),dpi=120)
visualize(vis.cpu())
plt.show()

"""### Fix intrinsic style codes"""

with torch.no_grad():
    img_gen, _ = generator([instyle[4:5].repeat(batch,1)], exstyle, input_is_latent=False, truncation=0.7, truncation_latent=0, 
                           use_res=True, interp_weights=[0.6]*7+[1]*11)
    
    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)
    
vis = torchvision.utils.make_grid(img_gen, batch, 1)
plt.figure(figsize=(10,10),dpi=120)
visualize(vis.cpu())
plt.show()

"""## Step 8 (GAN-demonium): Dimensionality Reduction on Extrinsic Styles"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload

# !pip install umap-learn scikit-learn 
# !pip install datashader colorcet pandas
from model.dsgandemonium import DualStyleGANdemonium as DSGANDemonium

## Step 8 (GAN-demonium): Dimensionality Reduction on Extrinsic Styles


cpu = lambda x: x.cpu().detach().numpy()
dsdemon = DSGANDemonium(1024, 512, 8, 2, res_index=6)
dsdemon.eval()
# ckpt = torch.load(MODEL_DIR, style_type , 'generator.pt', map_location=lambda storage, loc: storage)
ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'generator.pt'))
dsdemon.load_state_dict(ckpt["g_ema"])
dsdemon = dsdemon.to(device)
with torch.no_grad():
    _, instyle = encoder(I, randomize_noise=True, return_latents=True, 
                            z_plus_latent=True, return_z_plus_latent=True, resize=False)    
    
    latent = torch.tensor(exstyles[stylename]).to(device)
    exstyle = dsdemon.generator.style(latent)
    
    int_imgs = []
    vis_indicies = [1, 3, 5]
    for vis_index in vis_indicies:
        int_img_i, _ = dsdemon([instyle], exstyle, z_plus_latent=True, 
                               vis_index=vis_index, truncation=0.7, 
                               truncation_latent=0, use_res=True, 
                               interp_weights=[0.6]*7+[1]*11
                               )
        int_imgs.append(torch.clamp(int_img_i.detach(), -1, 1))
def visualize(img_arr, ax=None):
    if not ax:
        ax = plt.gca()
    ax.imshow(((img_arr.detach().numpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8), aspect="auto")
    ax.axis('off')
plt.clf(); fig = plt.gcf()
fig.set_size_inches(8, 8)
ax = fig.add_subplot(2, 2, 1)
ax.imshow(np.rot90(cpu(I)[0].swapaxes(0, 2), -1), aspect="auto")
ax.axis('off')
ax.set_title("Input Image")

for i, int_img_i in enumerate(int_imgs):
    # Take first 100 channels, treat as batch dimension for visualization.
    img_i_chans = int_img_i.swapaxes(0, 1)[:100]
    vis = torchvision.utils.make_grid(img_i_chans, nrows=10, padding=1)
    ax = fig.add_subplot(2, 2, i+2)
    visualize(vis.cpu(), ax)
    ax.set_title(f"Image Channels: {(i+1)*2} Layers")
fig.savefig("channel-vis", dpi=200)
import datashader as ds
import colorcet as cc
import datashader.transfer_functions as tf
import pandas as pd
import matplotlib.pyplot as plt

def render_embedding(data, **kwargs):
    size = min(data.shape[0] // 10, 100)
    canvas = ds.Canvas(100, 100, **kwargs)
    data = np.c_[data, np.arange(data.shape[0])]
    df = pd.DataFrame(data, columns=["x", "y", "i"])

    img = tf.shade(canvas.points(df, 'x', 'y'), how="eq_hist", cmap=plt.cm.viridis)
    img = tf.set_background(img, 'white')
    return img
# Aggregate extrinsic styles into a big matrix.
extrinsic = np.concatenate([style for style in exstyles.values()])
# Encode extrinsic styles into the intermediate latent space.
def encode_intermediate_style(styles):
    custyles = torch.from_numpy(styles).to(device)
    codes = np.dstack([cpu(generator.generator.style(style)) for style in custyles])
    codes = np.moveaxis(codes, (0, 1, 2), (1, 2, 0))
    return codes
# Partition the styles according to color and structure.
extrinsic_s = extrinsic.copy()
extrinsic_s[:,7:] = 0.0
extrinsic_c = extrinsic.copy()
extrinsic_c[:,:7] = 0.0
codes = encode_intermediate_style(extrinsic)
codes_s = encode_intermediate_style(extrinsic_s)
codes_c = encode_intermediate_style(extrinsic_c)
import umap
from sklearn import manifold

data = extrinsic.copy()
data = data.reshape(data.shape[0], -1)

# mapper = manifold.TSNE(n_components=2, perplexity=7.0)
mapper = umap.UMAP(n_neighbors=20, n_components=2)
embedding = mapper.fit_transform(data)

plt.clf()
fig = plt.gcf()
ax = fig.add_subplot()
ax.scatter(embedding[:,0], embedding[:,1], alpha=0.4)
ax.axis('off')
ax.set_title("UMAP Embedding of Encoded Extrinsic Style Images")
plt.show()
fig.savefig("UMAP-extstyle-images", dpi=200)
data = codes.reshape(codes.shape[0], -1)
# data = extrinsic.reshape(extrinsic.shape[0], -1)
n = data.shape[0]
_, s, _ = np.linalg.svd(data.reshape(n, -1))
s /= s.max()
exp_var = np.cumsum(s**2) / np.sum(s**2)
plt.clf(); fig = plt.gcf()
ax = fig.add_subplot()
ax.bar(np.arange(s.size), s, label="Singular Values")
ax.set_xlabel("Index")
ax.set_ylabel("Singular Value", color="C0")
ax.set_title("Encoded Latent Style Decomposition")
ax.tick_params(axis='y', labelcolor="C0")
ax2 = ax.twinx()
ax2.plot(np.arange(s.size), exp_var, "C1", label="Explained Variance")
ax2.tick_params(axis="y", labelcolor="C1")
ax2.set_ylabel("Explained Variance", color="C1")
fig.savefig("SVD-variance", dpi=200)